'''
ClassifiedRetriever

A general retriever to use a PriorGen classifier in retrieval.

The current build only uses nested sampling provided by dynesty.
'''

from .classifier import Classifier
import dynesty


class ClassifiedRetriever:
    def __init__(self, training_parameters, training_observables, n_classes=50,
                 n_components=15, n_bins=20):
        '''
        The ClassifiedRetriever is built to use a Classifier to run retrievals
        using the informed priors generated by the Classifer

        Parameters
        ----------
        training_parameters : array_like, shape (N, M)
            The physical parameter values for each point we are training the
            ML classifier on. N is the number of points, whilst M is the
            physical value for each parameter. These are all assumed to be in
            the same order. We assume that there are M variables in the model,
            and that none of them are constants.
        training_observables : array_like, shape (N, X)
            The observables associated with each of the parameters. We assume
            that the observables are 1D arrays where each entry is directly
            comparable. For example, it could be F(t), but where each entry is
            at the same value of t.
        n_classes : int, optional
            The number of different classes to use when training the classifier
            Default is 50.
        n_components : int, optional
            The number of principal components to use when conducting PCA on
            the observables. Default is 15.
        n_bins : int, optional
            The number of bins to split each maginalised parameter distribution
            into. The more bins you have, the more detail you will have on the
            shape of each class' prior. However, if you have too many, you will
            encounter issues of undersampling. Default is 20.
        '''
        self.classifier = Classifier(training_parameters, training_observables,
                                     n_classes, n_components, n_bins)

    def run_dynesty(self, data_to_fit, uncertainty, lnprob, nlive=200, bound='multi',
                    sample='auto', maxiter=None, maxcall=None,
                    **dynesty_kwargs):
        '''
        Runs nested sampling retrieval through Dynesty using the Classifier
        to inform priors

        Parameters
        ----------
        data_to_fit : array_like, shape (X,)
            The data you want to fit. Required for classification purposes
        lnprob : function
            A function which must be passed a set of parameters and returns
            their ln likelihood. Signature should be `lnprob(params)` where
            params is an array with shape (M, ). Note that you will need to
            have hard-coded the data and associated uncertainties into the
            `lnprob` function.
        nlive : int, optional
            The number of live points to use in the nested sampling. Default is
            200.
        bound : str, optional
            Method used to approximately bound the prior using the current set
            of live points. Conditions the sampling methods used to propose new
            live points. Choices are no bound ('none'), a single bounding
            ellipsoid ('single'), multiple bounding ellipsoids ('multi'), balls
            centered on each live point ('balls'), and cubes centered on each
            live point ('cubes'). Default is 'multi'.
        sample : str, optional
            Method used to sample uniformly within the likelihood constraint,
            conditioned on the provided bounds. Unique methods available are:
            uniform sampling within the bounds('unif'), random walks with fixed
            proposals ('rwalk'), random walks with variable (“staggering”)
            proposals ('rstagger'), multivariate slice sampling along preferred
            orientations ('slice'), “random” slice sampling along all
            orientations ('rslice'), and “Hamiltonian” slices along random
            trajectories ('hslice'). 'auto' selects the sampling method based
            on the dimensionality of the problem (from ndim). When ndim < 10,
            this defaults to 'unif'. When 10 <= ndim <= 20, this defaults to
            'rwalk'. When ndim > 20, this defaults to 'hslice' if a gradient is
            provided and 'slice' otherwise. 'rstagger' and 'rslice' are
            provided as alternatives for 'rwalk' and 'slice', respectively.
            Default is 'auto'.
        maxiter : int or None, optional
            The maximum number of iterations to run. If None, will run until
            stopping criterion is met. Default is None.
        maxcall : int or None, optional
            If not None, sets the maximum number of calls to the likelihood
            function. Default is None.
        **dynesty_kwargs : optional
            kwargs to be passed to the dynesty.NestedSampler() initialisation

        Returns
        -------
        results : dict
            The dynesty results dictionary, with the addition of the following
            attributes:
            weights - normalised weights for each sample
            cov - the covariance matrix
            uncertainties - the uncertainty on each fitted parameter,
                calculated from the square root of the diagonal of the
                covariance matrix.
        '''

        # First up, we need to define some variables for the Retriever
        # Number of dimensions we are retrieving
        n_dims = self.classifier.n_variables

        # Make the prior transform function
        prior_transform = self.classifier.create_dynesty_prior_transform(
            data_to_fit)

        # Set up and run the sampler here!!
        sampler = dynesty.NestedSampler(dynesty_lnlike, prior_transform,
                                ndims, bound=bound, sample=bound,
                                update_interval=float(ndims), nlive=nlive,
                                **dynesty_kwargs)

        sampler.run_nested(maxiter=maxiter, maxcall=maxcall)

        # Get some normalised weights
        results.weights = np.exp(results.logwt - results.logwt.max()) / \
            np.sum(np.exp(results.logwt - results.logwt.max()))

        # Calculate a covariance matrix for these results to get uncertainties
        cov = np.cov(results.samples, rowvar=False, aweights=results.weights)

        # Get the uncertainties from the diagonal of the covariance matrix
        diagonal = np.diag(cov)
        uncertainties = np.sqrt(diagonal)

        # Add the covariance matrix and uncertainties to the results object
        results.cov = cov
        results.uncertainties = uncertainties

        return results
